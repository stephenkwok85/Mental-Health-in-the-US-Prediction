%pip install scikit-learn
%pip install seaborn
%pip install plotly
%pip install shap
%pip install statsmodels

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, accuracy_score
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.ensemble import RandomForestClassifier
import plotly.express as px
import numpy as np 

file_path = 'Mental_Health_Care_in_the_Last_4_Weeks.csv'
df = pd.read_csv(file_path)

df.info()
df.head()

print(df.columns.tolist())

columns_to_keep = [
    'Indicator', 'Group', 'Subgroup', 'Time Period', 'Value'
]
df_reduced = df[columns_to_keep].copy()

df_reduced.dropna(subset=['Value'], inplace=True)

pivot_df = df_reduced.pivot_table(
    index=['Group', 'Subgroup'],
    columns='Indicator',
    values='Value',
    aggfunc='mean' 
).reset_index()

pivot_df.fillna(pivot_df.mean(numeric_only=True), inplace=True)

subgroup_labels = pivot_df[['Group', 'Subgroup']]

features = pivot_df.drop(columns=['Group', 'Subgroup'])

df_snapshot = df[
    (df['Phase'].str.contains('3')) & (df['Time Period Label'] == 'Dec 9 - Dec 21, 2020')
].copy()
print(df_snapshot['Group'].unique())
print("Rows:", df_snapshot.shape)
print("Missing Value count:", df_snapshot['Value'].isnull().sum())

df_pivot = df_snapshot.pivot(index=['Group','Subgroup'], 
                             columns='Indicator', values='Value')
df_pivot = df_pivot.reset_index()
df_pivot.head(6)

age_table = df_pivot[df_pivot['Group']=="By Age"].set_index('Subgroup')
age_table[['Took Prescription Medication for Mental Health And/Or Received Counseling or Therapy, Last 4 Weeks']].rename(
    columns={'Took Prescription Medication And/Or Therapy, Last 4 Weeks': 'Combined Care (%)'}) \
    .T

X = df_pivot[['Took Prescription Medication for Mental Health, Last 4 Weeks',
              'Received Counseling or Therapy, Last 4 Weeks',
              'Needed Counseling or Therapy But Did Not Get It, Last 4 Weeks']]
kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
df_pivot['Cluster'] = kmeans.labels_
print(df_pivot[['Group','Subgroup','Cluster']].head(8))

X = df_pivot[['Took Prescription Medication for Mental Health, Last 4 Weeks',
              'Received Counseling or Therapy, Last 4 Weeks',
              'Needed Counseling or Therapy But Did Not Get It, Last 4 Weeks']].fillna(0)
y = (df_pivot['Took Prescription Medication for Mental Health And/Or Received Counseling or Therapy, Last 4 Weeks'] >= df_pivot['Took Prescription Medication for Mental Health And/Or Received Counseling or Therapy, Last 4 Weeks'].median()).astype(int)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

logreg = LogisticRegression().fit(X_train, y_train)
y_pred = logreg.predict(X_test)
print("Logistic accuracy:", accuracy_score(y_test, y_pred))
print("Coefficients:")
for name, coef in zip(X.columns, logreg.coef_[0]):
    print(f"  {name}: {coef:.3f}")

rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)
print("RF accuracy:", accuracy_score(y_test, rf.predict(X_test)))
importances = rf.feature_importances_
print("Feature importances:")
for name, imp in zip(X.columns, importances):
    print(f"  {name}: {imp:.3f}")

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

pca = PCA(n_components=2)
pca_components = pca.fit_transform(scaled_features)

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(pca_components)

result_df = subgroup_labels.copy()
result_df['PC1'] = pca_components[:, 0]
result_df['PC2'] = pca_components[:, 1]
result_df['Cluster'] = clusters

result_df.head()

sns.set(style="whitegrid", rc={"figure.figsize": (10, 6)})

plt.figure(figsize=(10, 6))
scatter = sns.scatterplot(
    x='PC1', y='PC2', hue='Cluster', style='Group',
    data=result_df, palette='Set2', s=100
)

for i in range(result_df.shape[0]):
    plt.text(
        result_df['PC1'][i] + 0.05, result_df['PC2'][i],
        result_df['Subgroup'][i], fontsize=8
    )

plt.title('Clusters of Mental Health Care Behavior by Demographic Group')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

loadings = pd.DataFrame(pca.components_.T, index=features.columns, columns=['PC1', 'PC2'])

top_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head(5)
top_pc2 = loadings['PC2'].abs().sort_values(ascending=False).head(5)

top_contributors = pd.DataFrame({
    'Top PC1 Features': top_pc1.index,
    'PC1 Contribution': top_pc1.values,
    'Top PC2 Features': top_pc2.index,
    'PC2 Contribution': top_pc2.values
})
print(top_contributors)

features_with_clusters = pd.DataFrame(scaled_features, columns=features.columns)
features_with_clusters['Cluster'] = clusters

cluster_feature_means = features_with_clusters.groupby('Cluster').mean()
print(cluster_feature_means)

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', palette='viridis', data=result_df)
plt.title('K-Means Clustering on PCA Components')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Analyze cluster characteristics
cluster_analysis = result_df.groupby('Cluster').agg({
    'PC1': 'mean',
    'PC2': 'mean',
    'Group': 'nunique',
    'Subgroup': 'nunique'
})

print("Cluster Analysis:")
print(cluster_analysis)

# Heatmap of average feature values by cluster
plt.figure(figsize=(12, 6))
sns.heatmap(cluster_feature_means, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Average Scaled Feature Values per Cluster')
plt.xlabel('Mental Health Indicators')
plt.ylabel('Cluster')
plt.tight_layout()
plt.show()

top_features = top_pc1.index.tolist() + top_pc2.index.tolist()
top_features = list(set(top_features))

plt.figure(figsize=(12, 6))
cluster_feature_means[top_features].T.plot(kind='bar', figsize=(12, 6))
plt.title('Top PCA-Contributing Features Across Clusters')
plt.xlabel('Mental Health Indicator')
plt.ylabel('Average (Scaled)')
plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Parallel Coordinates Plot
fig = px.parallel_coordinates(
    pivot_df,
    color_continuous_scale=px.colors.sequential.Inferno,
    labels=pivot_df.columns 
)
fig.show()

data = df_reduced.copy()   

categorical_features = ['Group', 'Subgroup'] 
data = pd.get_dummies(data, columns=categorical_features, drop_first=True) 

print("Available indicators in 'Indicator' column:")
print(data['Indicator'].unique()) 

target_indicator = 'Received Counseling or Therapy, Last 4 Weeks' 
data['Target'] = (df_reduced['Indicator'] == target_indicator).astype(int) 
data.dropna(subset=['Target'], inplace=True) 

y = data['Target']
X = data.drop(columns=['Indicator', 'Time Period', 'Value', 'Target'])  

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
model = LogisticRegression(solver='liblinear') 
model.fit(X_train, y_train) 

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print(f"Accuracy: {model.score(X_test, y_test):.2f}")

pca_for_viz = PCA(n_components=2)
pca_data = pca_for_viz.fit_transform(scaled_features) 

# DBSCAN Clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(scaled_features)

# Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=4)
agglo_labels = agglo.fit_predict(scaled_features)

result_df['DBSCAN_Cluster'] = dbscan_labels
result_df['Agglo_Cluster'] = agglo_labels

plt.figure(figsize=(12, 5))

# DBSCAN
plt.subplot(1, 2, 1)
unique_labels_dbscan = np.unique(dbscan_labels)
colors_dbscan = [plt.cm.viridis(i/float(len(unique_labels_dbscan)-1)) for i in range(len(unique_labels_dbscan))]
for k, col in zip(unique_labels_dbscan, colors_dbscan):
    if k == -1: # Noise points
        col = [0, 0, 0, 1]
    class_member_mask = (dbscan_labels == k)
    xy = pca_data[class_member_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=3)
plt.title('DBSCAN Clustering on PCA Components')

# Agglomerative
plt.subplot(1, 2, 2)
unique_labels_agglo = np.unique(agglo_labels)
colors_agglo = [plt.cm.viridis(i/float(len(unique_labels_agglo)-1)) for i in range(len(unique_labels_agglo))]
for k, col in zip(unique_labels_agglo, colors_agglo):
    class_member_mask = (agglo_labels == k)
    xy = pca_data[class_member_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=3)
plt.title('Agglomerative Clustering on PCA Components')

plt.tight_layout()
plt.show()

# Supervised Classification: Predict High vs Low Care-Seeking Groups 
target_col = 'Received Counseling or Therapy, Last 4 Weeks'
median_value = pivot_df[target_col].median()
pivot_df['high_care_seeking'] = (pivot_df[target_col] > median_value).astype(int)

X = pivot_df.drop(columns=['Group', 'Subgroup', target_col, 'high_care_seeking'])
y = pivot_df['high_care_seeking']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, confusion_matrix

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

results = {}
plt.figure(figsize=(8, 6))

for name, model in models.items():
    # Fit on training data
    model.fit(X_train_scaled, y_train)
    # Predict on test data
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1]

    # Compute metrics
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    results[name] = {'accuracy': acc, 'roc_auc': auc}

    # Print classification report
    print(f"--- {name} ---")
    print(f"Test Accuracy: {acc:.3f}, ROC AUC: {auc:.3f}")
    print(classification_report(y_test, y_pred))

    # Plot ROC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.2f})")

# Plot baseline
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Care-Seeking Classification')
plt.legend()
plt.tight_layout()
plt.show()

# Feature importances from Random Forest

importances = pd.DataFrame({
    'feature': X.columns,
    'importance': models['Random Forest'].feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(importances['feature'], importances['importance'])
plt.xlabel('Importance')
plt.title('Random Forest Feature Importances')
plt.tight_layout()
plt.show()

from sklearn.model_selection import StratifiedKFold, cross_val_score

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}
for name, model in models.items():
    acc_scores = cross_val_score(model, scaler.fit_transform(X), y, cv=skf, scoring='accuracy')
    auc_scores = cross_val_score(model, scaler.fit_transform(X), y, cv=skf, scoring='roc_auc')
    cv_results[name] = {
        'acc_mean': acc_scores.mean(), 'acc_std': acc_scores.std(),
        'auc_mean': auc_scores.mean(), 'auc_std': auc_scores.std()
    }
    print(f"{name}: Accuracy = {cv_results[name]['acc_mean']:.3f} ± {cv_results[name]['acc_std']:.3f}, "
          f"AUC = {cv_results[name]['auc_mean']:.3f} ± {cv_results[name]['auc_std']:.3f}")


# 1. Correlation Heatmap
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import scipy.stats as stats
import statsmodels.api as sm

# Compute correlation matrix including target
df_corr = pivot_df.drop(columns=['Group', 'Subgroup']).corr()
plt.figure(figsize=(12, 10))
sns.heatmap(df_corr, annot=True, fmt='.2f', cmap='vlag', center=0)
plt.title('Correlation Matrix of Features and Target')
plt.tight_layout()
plt.show()

# 2. PCA Visualization
pca = PCA(n_components=2, random_state=42)
components = pca.fit_transform(scaler.fit_transform(X))
plt.figure(figsize=(8, 6))
plt.scatter(components[:,0], components[:,1], c=y, cmap='coolwarm', s=50)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA: PC1 vs PC2 Colored by Care-Seeking')
plt.colorbar(label='High Care-Seeking')
plt.tight_layout()
plt.show()

# 3. t-SNE Visualization
tsne = TSNE(n_components=2, random_state=42)
tsne_proj = tsne.fit_transform(scaler.fit_transform(X))
plt.figure(figsize=(8,6))
plt.scatter(tsne_proj[:,0], tsne_proj[:,1], c=y, cmap='coolwarm', s=50)
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.title('t-SNE Projection Colored by Care-Seeking')
plt.colorbar(label='High Care-Seeking')
plt.tight_layout()
plt.show()

# 4. Clustering Stability for KMeans and Agglomerative
for method_name, clusterer in [('KMeans', KMeans(n_clusters=2, random_state=42)),
                               ('Agglomerative', AgglomerativeClustering(n_clusters=2))]:
    labels = clusterer.fit_predict(scaler.fit_transform(X))
    sil = silhouette_score(scaler.fit_transform(X), labels)
    print(f"{method_name} silhouette score: {sil:.3f}")
    cross_tab = pd.crosstab(labels, pivot_df['high_care_seeking'])
    print(cross_tab, "\n")

# 5. Mann-Whitney U Test for Top Continuous Features
# Identify top 5 features by absolute correlation with target
top_feats = df_corr[target_col].drop(target_col).abs().sort_values(ascending=False).index[:5]
print('Top features for tests:', list(top_feats))
for feat in top_feats:
    grp1 = pivot_df[pivot_df['high_care_seeking']==1][feat]
    grp0 = pivot_df[pivot_df['high_care_seeking']==0][feat]
    u_stat, p = stats.mannwhitneyu(grp1, grp0, alternative='two-sided')
    print(f"Mann-Whitney U test for {feat}: U={u_stat:.1f}, p={p:.3f}")

# 6. Logistic Regression Summary via Statsmodels
X_sm = sm.add_constant(X)
model_sm = sm.Logit(pivot_df['high_care_seeking'], X_sm).fit(disp=False)
print(model_sm.summary())

# 7. Hierarchical Clustering Dendrogram
from scipy.cluster.hierarchy import linkage, dendrogram

linked = linkage(scaler.fit_transform(X), method='ward')
plt.figure(figsize=(10, 5))
dendrogram(linked, labels=pivot_df['high_care_seeking'].values, leaf_rotation=90)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index (label = care-seeking)')
plt.ylabel('Distance')
plt.tight_layout()
plt.show()

# 8. Updated Boxplots & Violin Plots for Top Features with jitter
plt.figure(figsize=(15, 5))
for i, feat in enumerate(top_feats[:3], 1):
    plt.subplot(1, 3, i)
    sns.violinplot(x='high_care_seeking', y=feat, data=pivot_df, palette='Set2', inner='quartile')
    sns.stripplot(x='high_care_seeking', y=feat, data=pivot_df, color='k', size=3, jitter=0.2, alpha=0.7)
    plt.xticks([0,1], ['Low', 'High'])
    plt.xlabel('Care-Seeking')
    plt.title(feat)
    plt.tight_layout()
plt.suptitle('Top Features Distribution by Care-Seeking with Jittered Points', y=1.02)
plt.show()
